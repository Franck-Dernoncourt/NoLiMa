{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from copy import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "booknames = os.listdir(\"./books/II-clean\")\n",
    "\n",
    "for bookname in booknames:\n",
    "    if not bookname.endswith(\".txt\"):\n",
    "        continue\n",
    "    with open(\"./books/II-clean/\" + bookname, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove all punctionation except dashes\n",
    "    newtext = re.sub(r'[^\\w\\s-]', '', text)\n",
    "\n",
    "    # Split text into words\n",
    "    words.update(newtext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "model = AutoModel.from_pretrained('facebook/contriever').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(words)\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer\n",
    "all_embeddings = None\n",
    "for i in range(0, len(word_list), 512):\n",
    "    inputs = tokenizer(word_list[i:i+512], padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\n",
    "\n",
    "    # Compute token embeddings\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Mean pooling\n",
    "    def mean_pooling(token_embeddings, mask):\n",
    "        mask[:, 0] = 0  # Ignore the [CLS] token\n",
    "        lengths = mask.sum(dim=1)\n",
    "        # Ignore the [SEP] token\n",
    "        for i in range(len(mask)):\n",
    "            mask[i, lengths[i]] = 0\n",
    "\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "    if all_embeddings is None:\n",
    "        all_embeddings = embeddings.cpu().detach().numpy()\n",
    "    else:\n",
    "        all_embeddings = np.concatenate((all_embeddings, embeddings.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "\n",
    "needle_words = [\"hague\", \"frankfurt\", \"oper\", \"opera\", \"dresden\", \"markt\", \"Helsinki\", \"Uusimaa\", \"milk\", \"lactose\", \"intolerant\",\n",
    "               \"cappuccino\", \"caffè\", \"mocha\", \"caffe\", \"fish\", \"egg\", \"omelette\", \"france\", \"iran\", \"south africa\", \"mauritshuis\", \"museum\", \"netherland\",\n",
    "               \"madrid\", \"prado\", \"museo\", \"vegan\", \"spain\", \"Musée\", \"marmottan\",  \"monet\", \"paris\"]\n",
    "\n",
    "# Apply tokenizer\n",
    "inputs = tokenizer(needle_words, padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\n",
    "# Compute token embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "needle_embeddings = mean_pooling(outputs[0], inputs['attention_mask']).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (all_embeddings @ needle_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 25\n",
    "for i, needle_word in enumerate(needle_words):\n",
    "    similar_words = [word_list[j] for j in similarity[:, i].argsort()[-TOP_K:][::-1]]\n",
    "    print(f\"Words most similar to '{needle_word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_distractor_words = [\"amsterdam\", \"germany\", \"rhine\", \"berlin\", \"orchestra \", \" oper-\", \"finland\", \"achtung\"\n",
    "                             \"dairy\", \"cappuccino\", \"coffee\", \"caffein\", \"starbucks\", \" latte \", \" tuna\", \"shrimp\", \" crab\", \" omelet\",\n",
    "                             \"french\", \"Le havre\", \"persia\", \" iranoi\", \" amir\", \" iraq\", \" kurd\", \"africa\", \"cape town\", \"valencia\", \"espanol\", \" diet\",\n",
    "                             \"spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booknames = os.listdir(\"./books/II-clean\")\n",
    "\n",
    "needle_words_w_spacing = [\"hague\", \"frankfurt\", \"oper \", \" opera \", \"dresden\", \"markt \", \"Helsinki\", \"Uusimaa\", \"milk\", \"lactose\", \"intolerant\",\n",
    "               \"cappuccino\", \"caffè\", \"mocha\", \"caffe\", \"fish\", \" egg\", \"omelette\", \"france\", \" iran\", \"south africa\", \"mauritshuis\", \"museum\", \"netherland\",\n",
    "               \"madrid\", \"prado\", \"museo\", \"vegan\", \"spain\", \"musée\", \"marmottan\",  \" monet \", \" paris\"]\n",
    "\n",
    "distractors = needle_words_w_spacing + relevant_distractor_words\n",
    "bypass = [\"proper\", \"selfish\", \"doper\", \"cooper\", \"developer\", \"trooper\", \"mirand\", \"standoffish\", \"dieter\"]\n",
    "\n",
    "W = 12\n",
    "BYPASS_W = 6\n",
    "\n",
    "for bookname in booknames:\n",
    "    if not bookname.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    with open(\"./books/II-clean/\" + bookname, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(\"Checking book: \" + bookname)\n",
    "    spans_to_remove = []\n",
    "    for distractor in distractors:\n",
    "        if \" \" == distractor[0] and \" \" == distractor[-1]:\n",
    "            pattern = distractor\n",
    "        elif \" \" == distractor[0]:\n",
    "            pattern = rf'{distractor}\\w*\\b'\n",
    "        elif \" \" == distractor[-1]:\n",
    "            pattern = rf'\\b\\w*{distractor}'\n",
    "        else:\n",
    "            pattern = rf'\\b\\w*{distractor}\\w*\\b'\n",
    "\n",
    "        for match in re.finditer(pattern, text.lower()):\n",
    "            if any(b in text[match.start()-BYPASS_W:match.end()+BYPASS_W].lower() for b in bypass):\n",
    "                continue\n",
    "\n",
    "            print(\"Found match in: \" + text[match.start()-W:match.end()+W].replace(\"\\n\", \" \"))\n",
    "            start = max(text[:match.start()-W].rfind(\" \"), text[:match.start()-W].rfind(\"\\n\"))\n",
    "            end = min(text[match.end()+W:].find(\" \"), text[match.end()+W:].find(\"\\n\")) + match.end()+W\n",
    "            spans_to_remove.append((start, end))\n",
    "    # break\n",
    "\n",
    "    spans_to_remove.sort(key=lambda x: x[0])\n",
    "    # Merge overlapping spans\n",
    "    i = 0\n",
    "    while i < len(spans_to_remove) - 1:\n",
    "        if spans_to_remove[i][1] >= spans_to_remove[i+1][0]:\n",
    "            spans_to_remove[i] = (spans_to_remove[i][0], spans_to_remove[i+1][1])\n",
    "            spans_to_remove.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    redacted_book = copy(text)\n",
    "    shift = 0\n",
    "    for start, end in spans_to_remove:\n",
    "        redacted_book = redacted_book[:start-shift] + \"... \" + redacted_book[end-shift:]\n",
    "        shift += end-start-4\n",
    "    \n",
    "    with open(\"./books/II-clean/wo_dist/\" + bookname, \"w\") as f:\n",
    "        f.write(redacted_book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
